{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imdb data\n",
    "from keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(25000,)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# look at data\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at one sequence\n",
    "train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dictionary that maps words to numbers\n",
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that maps numbers to words\n",
    "reverse_word_index = dict([(key,value) for (value, key) in word_index.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"? this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had ? working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how ? this is to watch save yourself an hour a bit of your life\""
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# reshape one comment from numbers to actual sentences\n",
    "comment = \" \".join(reverse_word_index.get(i-3, '?') for i in train_data[2]) \n",
    "comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "import numpy as np\n",
    "def vectorize_sequences(sequences , dimesnsion=10000):\n",
    "    results = np.zeros((len(sequences), dimesnsion))\n",
    "    for i , sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation data\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( optimizer= 'rmsprop',\n",
    "                            loss = 'binary_crossentropy',\n",
    "                            metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 15000 samples, validate on 10000 samples\nEpoch 1/20\n15000/15000 [==============================] - 7s 436us/step - loss: 0.5375 - accuracy: 0.7903 - val_loss: 0.4079 - val_accuracy: 0.8711\nEpoch 2/20\n15000/15000 [==============================] - 6s 374us/step - loss: 0.3274 - accuracy: 0.8979 - val_loss: 0.3544 - val_accuracy: 0.8558\nEpoch 3/20\n15000/15000 [==============================] - 5s 314us/step - loss: 0.2382 - accuracy: 0.9227 - val_loss: 0.2824 - val_accuracy: 0.8914\nEpoch 4/20\n15000/15000 [==============================] - 5s 344us/step - loss: 0.1856 - accuracy: 0.9415 - val_loss: 0.2750 - val_accuracy: 0.8902\nEpoch 5/20\n15000/15000 [==============================] - 5s 305us/step - loss: 0.1490 - accuracy: 0.9527 - val_loss: 0.2996 - val_accuracy: 0.8815\nEpoch 6/20\n15000/15000 [==============================] - 5s 322us/step - loss: 0.1241 - accuracy: 0.9600 - val_loss: 0.3136 - val_accuracy: 0.8820\nEpoch 7/20\n15000/15000 [==============================] - 5s 328us/step - loss: 0.1053 - accuracy: 0.9686 - val_loss: 0.3154 - val_accuracy: 0.8835\nEpoch 8/20\n15000/15000 [==============================] - 5s 318us/step - loss: 0.0849 - accuracy: 0.9768 - val_loss: 0.3204 - val_accuracy: 0.8832\nEpoch 9/20\n15000/15000 [==============================] - 5s 311us/step - loss: 0.0699 - accuracy: 0.9811 - val_loss: 0.3625 - val_accuracy: 0.8788\nEpoch 10/20\n15000/15000 [==============================] - 5s 328us/step - loss: 0.0581 - accuracy: 0.9853 - val_loss: 0.3703 - val_accuracy: 0.8781\nEpoch 11/20\n15000/15000 [==============================] - 4s 270us/step - loss: 0.0459 - accuracy: 0.9898 - val_loss: 0.4286 - val_accuracy: 0.8677\nEpoch 12/20\n15000/15000 [==============================] - 4s 298us/step - loss: 0.0372 - accuracy: 0.9923 - val_loss: 0.4351 - val_accuracy: 0.8715\nEpoch 13/20\n15000/15000 [==============================] - 5s 301us/step - loss: 0.0295 - accuracy: 0.9934 - val_loss: 0.4658 - val_accuracy: 0.8738\nEpoch 14/20\n15000/15000 [==============================] - 5s 336us/step - loss: 0.0243 - accuracy: 0.9955 - val_loss: 0.4926 - val_accuracy: 0.8724\nEpoch 15/20\n15000/15000 [==============================] - 4s 275us/step - loss: 0.0177 - accuracy: 0.9975 - val_loss: 0.5261 - val_accuracy: 0.8702\nEpoch 16/20\n15000/15000 [==============================] - 4s 295us/step - loss: 0.0153 - accuracy: 0.9975 - val_loss: 0.5560 - val_accuracy: 0.8704\nEpoch 17/20\n15000/15000 [==============================] - 5s 318us/step - loss: 0.0114 - accuracy: 0.9989 - val_loss: 0.5886 - val_accuracy: 0.8690\nEpoch 18/20\n15000/15000 [==============================] - 4s 287us/step - loss: 0.0094 - accuracy: 0.9986 - val_loss: 0.6214 - val_accuracy: 0.8686\nEpoch 19/20\n15000/15000 [==============================] - 4s 277us/step - loss: 0.0050 - accuracy: 0.9998 - val_loss: 0.6508 - val_accuracy: 0.8668\nEpoch 20/20\n15000/15000 [==============================] - 4s 295us/step - loss: 0.0104 - accuracy: 0.9972 - val_loss: 0.6818 - val_accuracy: 0.8658\n"
    }
   ],
   "source": [
    "history = model.fit(partial_x_train, partial_y_train, epochs= 20, batch_size= 512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bitec555a7956d24fa3b3a7cf8e25037032",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}